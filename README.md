## Lumina Controlnetè®­ç»ƒè„šæœ¬

è¿™æ˜¯ä¸€ä¸ªæœªå®Œå·¥çš„è„šæœ¬ï¼ŒåŸºäº[sdbds/sd-scripts](https://github.com/sdbds/sd-scripts/tree/lumina)ï¼ˆä¸€ä¸ª[Kohya_ss/sd-scripts](https://github.com/kohya-ss/sd-scripts)çš„æ”¯æŒLuminaçš„åˆ†æ”¯ï¼‰ä¿®æ”¹ã€‚å½“å‰æœªå®Œå·¥ï¼Œç»æµ‹è¯•å­˜åœ¨å¤§é‡BUGï¼Œå»ºè®®è°¨æ…ä½¿ç”¨ã€‚


#### å‡†å¤‡æ•°æ®é›†

```âœ¨ Lumina2Dataset (æ•°æ®é›†)
â”ƒ
â” â”€â”€â”€ğŸ“‚ image (æ–‡ä»¶å¤¹1: å›¾åƒä¸çµæ„Ÿ)
â”ƒ   â”ƒ
â”ƒ   â” â”€â”€ ğŸ¨ 1.png  (ä¸€å¼ ç¾ä¸½çš„ç”»)
â”ƒ   â” â”€â”€ ğŸ“ 1.txt  ("ä¸€åªçŒ«å’ªååœ¨çª—è¾¹ï¼Œæœ›ç€æ˜Ÿç©º")
â”ƒ   â”ƒ
â”ƒ   â” â”€â”€ ğŸ¨ 2.png  (å¦ä¸€å¼ å¥‡å¦™çš„å›¾)
â”ƒ   â” â”€â”€ ğŸ“ 2.txt  ("èµ›åšæœ‹å…‹åŸå¸‚çš„é›¨å¤œï¼Œéœ“è™¹ç¯é—ªçƒ")
â”ƒ   â”ƒ
â”ƒ   â” â”€â”€ ğŸ¨ 3.png  (......)
â”ƒ   â” â”€â”€ ğŸ“ 3.txt  (......)
â”ƒ   â”ƒ
â”ƒ   â”–â”€â”€ ... (æ›´å¤šæˆå¯¹çš„å›¾åƒå’Œæè¿°)
â”ƒ
â” â”€â”€â”€ğŸ“‚ condition (æ–‡ä»¶å¤¹2: ç»“æ„ä¸å¼•å¯¼)
â”ƒ   â”ƒ
â”ƒ   â” â”€â”€ ğŸ“ 1.png  (ä¸ç”»1å¯¹åº”çš„ç»“æ„å›¾)
â”ƒ   â” â”€â”€ ğŸ“ 2.png  (ä¸ç”»2å¯¹åº”çš„ç»“æ„å›¾)
â”ƒ   â” â”€â”€ ğŸ“ 3.png  (......)
â”ƒ   â”–â”€â”€ ... (æ›´å¤šä¸å›¾åƒä¸€ä¸€å¯¹åº”çš„å¼•å¯¼å›¾)
â”ƒ
â”–â”€â”€ ... (å…¶ä»–æ–‡ä»¶å¤¹)
```

#### ä½¿ç”¨æ­¥éª¤ï¼›
1ã€éƒ¨ç½²å¥½kohya_ssç¯å¢ƒã€‚

2ã€æŒ‰ç…§ç›®å½•ç»“æ„ï¼ŒæŠŠæ–‡ä»¶å¤åˆ¶åˆ°kohya_ssçš„sd-scriptsç›®å½•ä¸‹ã€‚

3ã€ä¸‹è½½Lumina diffusion modelã€CLIPï¼ˆgemma2-2bï¼‰ã€VAEï¼ˆfluxçš„aeï¼‰ã€tokenizerï¼ˆGemma2-2bï¼‰ã€‚

4ã€ä¿®æ”¹strategy_lumina.pyä¸­çš„GEMMA_IDä¸ºtokenizerçš„æœ¬åœ°è·¯å¾„æˆ–huggingface Model IDã€‚

5ã€ä¿®æ”¹å‘½ä»¤ä¸­çš„å„ç§å‚æ•°ï¼ˆdatasetè·¯å¾„ã€conditionè·¯å¾„ã€æ¨¡å‹è·¯å¾„ç­‰ï¼‰ã€‚

6ã€åœ¨ç»ˆç«¯é‡Œè¿›å…¥kohya_ssçš„è™šæ‹Ÿç¯å¢ƒï¼Œå¹¶è¿›å…¥sd-scriptsç›®å½•ï¼Œæ‰§è¡Œè®­ç»ƒå‘½ä»¤ã€‚

**ï¼ˆä¸‹é¢çš„å‘½ä»¤åªæ˜¯æ¼”ç¤ºæ ¼å¼ï¼Œå…¶ä¸­å­˜åœ¨ä¸€äº›é”™è¯¯æˆ–ä¸å½“çš„éƒ¨åˆ†ï¼‰**

```python lumina_controlnet.py      --console_log_level INFO     --console_log_file "D:\\kohya_ss\\logs\\train\\lumina2.log"     --pretrained_model_name_or_path    "D:\\LuminaCtTrain\\models\\diffusion_model\\NetaYumev35_pretrained_unet.safetensors"     --gemma2 "D:\\LuminaCtTrain\\models\\text_encoder\\gemma_2_2b_fp16.safetensors" --tokenizer_cache_dir "D:\\LuminaCtTrain\\models\\text_encoder"   --gemma2_max_token_length 4096     --ae "D:\\LuminaCtTrain\\models\\vae\\flux_ae.safetensors"     --train_data_dir "D:\\kohya_ss\\dataset\\Lumina2Dataset\\image"     --conditioning_data_dir "D:\\kohya_ss\\dataset\\Lumina2Dataset\\deepth"         --resolution 1024     --train_batch_size 1     --caption_extension "txt"     --output_dir  "D:\\kohya_ss\\outputs\\lumina2Deepth"     --output_name "lumina2_deepth_controlnet"     --save_every_n_steps 100     --xformers     --sdpa     --max_train_steps 20000     --seed 23672323     --mixed_precision fp16     --full_bf16  --gradient_checkpointing    --clip_skip 2     --metadata_author "æ˜ŸæœˆStarMoon"       --optimizer_type AdamW8bit     --learning_rate 5e-6     --lr_scheduler cosine     --lr_warmup_steps 100     --save_model_as safetensors```


#### å‘½ä»¤è¯´æ˜ï¼š

* é¢„è®­ç»ƒUNetæ¨¡å‹: D:\LuminaCtTrain\models\diffusion_model\NetaYumev35_pretrained_unet.safetensors
    - --pretrained_model_name_or_path

* Gemma2 æ–‡æœ¬ç¼–ç å™¨: D:\LuminaCtTrain\models\text_encoder\gemma_2_2b_fp16.safetensors
    - --gemma2

* Tokenizer ç¼“å­˜ç›®å½•: D:\LuminaCtTrain\models\text_encoder
    - --tokenizer_cache_dir
* AE (Autoencoder) æ¨¡å‹: D:\LuminaCtTrain\models\vae\flux_ae.safetensors
    - --ae
* è®­ç»ƒå›¾åƒæ•°æ®: D:\kohya_ss\dataset\Lumina2Dataset\image
    - --train_data_dir
* æ¡ä»¶å›¾åƒæ•°æ® (æ·±åº¦å›¾): D:\kohya_ss\dataset\Lumina2Dataset\deepth
    - --conditioning_data_dir
* è¾“å‡ºç›®å½•: D:\kohya_ss\outputs\lumina2Deepth
    - --output_dir
* æ—¥å¿—æ–‡ä»¶: D:\kohya_ss\logs\train\lumina2.log
    - --console_log_file
* åˆ†è¾¨ç‡: 1024
    - --resolution
* æ‰¹æ¬¡å¤§å° (Batch Size): 1
    - --train_batch_size
* æœ€å¤§è®­ç»ƒæ­¥æ•°: 20000
    - --max_train_steps
* æ–‡æœ¬æ ‡ç­¾æ–‡ä»¶æ‰©å±•å: txt
    - --caption_extension
* Gemma2 æœ€å¤§Tokené•¿åº¦: 4096
    - --gemma2_max_token_length
* Clip Skip: 2
    - --clip_skip
* éšæœºç§å­: 23672323
- --seed

* è¾“å‡ºæ¨¡å‹åç§°: lumina2_deepth_controlnet
    - --output_name
* æ¨¡å‹ä¿å­˜æ ¼å¼: safetensors
    - --save_model_as
* ä¿å­˜é¢‘ç‡: æ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡
- --save_every_n_steps
* å…ƒæ•°æ®ä½œè€…: æ˜ŸæœˆStarMoon
- --metadata_author

* æ··åˆç²¾åº¦: bf16
    - --mixed_precision
* å¯ç”¨ xformers: æ˜¯
- --xformers
* å¯ç”¨ SDPA (Scaled Dot Product Attention): æ˜¯
- --sdpa
* å¯ç”¨å®Œæ•´ BF16: æ˜¯
- --full_bf16
* å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹: æ˜¯
- --gradient_checkpointing


* ä¼˜åŒ–å™¨ç±»å‹: AdamW
    - --optimizer_type
* å­¦ä¹ ç‡: 5e-6 (å³ 0.000005)
    - --learning_rate
* å­¦ä¹ ç‡è°ƒåº¦å™¨: cosine (ä½™å¼¦é€€ç«)
    - --lr_scheduler
* å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°: 100
    - --lr_warmup_steps

* æ§åˆ¶å°æ—¥å¿—çº§åˆ«: INFO
    - --console_log_level

#### å½“å‰é—®é¢˜

ç”±äºæˆ‘æœ¬äººèƒ½åŠ›ä¸è¶³ï¼Œæ— æ³•è§£å†³è®­ç»ƒçˆ†æ˜¾å­˜çš„é—®é¢˜:
```torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 778.00 MiB. GPU 0 has a total capacity of 47.38 GiB of which 612.69 MiB is free. Including non-PyTorch memory, this process has 46.78 GiB memory in use. Of the allocated memory 44.78 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|                                                                                         | 0/20000 [00:01<?, ?it/s]
root@autodl-container-f4ac42b1b8-d9c9196a:~/kohya_ss#
```

å°è¯•è¿‡å¯ç”¨adamw8bitä¼˜åŒ–å™¨ï¼Œ--cache_latentså’Œ--cache_latents_to_diskç­‰ä¼˜åŒ–æªæ–½ï¼Œä½†æ˜¯å‡å‡ºç°å’±éš¾ä»¥è§£å†³çš„æŠ¥é”™ã€‚


#### è‡´è°¢
- [sdbds/sd-scripts](https://github.com/sdbds/sd-scripts/tree/lumina)
- [Kohya_ss/sd-scripts](https://github.com/kohya-ss/sd-scripts)
- [Gemini 2.5 Pro in Google AI Studio](https://aistudio.google.com/prompts/new_chat)


